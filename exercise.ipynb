{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714583c5ec019154",
   "metadata": {},
   "source": [
    "### A Tutorial on Modern Hopfield Networks\n",
    "\n",
    "#### Before You Begin\n",
    "\n",
    "This tutorial explores the concepts introduced in the paper [**\"Dense Associative Memory for Pattern Recognition\"**](https://arxiv.org/abs/1606.01164) by Dmitry Krotov and John J. Hopfield (2016). You will need to implement key formulas from the paper to complete the exercises. Reading it beforehand is highly recommended to understand the theoretical foundation.\n",
    "\n",
    "#### What are Hopfield Networks?\n",
    "\n",
    "Hopfield Networks are a form of recurrent neural network that serve as **associative memory**. The core idea is simple: you store a set of patterns in the network, and when you later provide a partial or corrupted version of one of those patterns, the network can retrieve the original, complete pattern.\n",
    "\n",
    "Think of it like human memory. If you see a blurry picture of a cat, your brain can often fill in the missing details and recognize it as a cat. Hopfield networks attempt to model this process mathematically.\n",
    "\n",
    "#### From Classical to Modern\n",
    "\n",
    "The **Classical Hopfield Network**, proposed by John Hopfield in 1982, had a significant limitation: its **storage capacity**. It could only reliably store a small number of patterns, approximately 14% of the number of neurons in the network ($K_{max} \\approx 0.14N$).\n",
    "\n",
    "This is where **Modern Hopfield Networks** come in. Based on work by Krotov and Hopfield (2016), these networks use a generalized energy function that allows for a much greater storage capacity, scaling polynomially with the number of neurons ($K_{max} \\propto N^{n-1}$)[c. This enhancement makes them capable of tackling more complex problems, like recalling high-resolution images.\n",
    "\n",
    "In this tutorial, we will:\n",
    "\n",
    "1.  Implement and understand the Classical Hopfield Network.\n",
    "2.  Demonstrate its limitations on a simple image-recall task.\n",
    "3.  Implement the Modern Hopfield Network.\n",
    "4.  Show its superior performance on the same task.\n",
    "\n",
    "Let's start with some utility functions for handling and displaying images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0fc937",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Self\n",
    "from random import randint\n",
    "from collections.abc import Callable\n",
    "\n",
    "import PIL.Image\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da0a20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image: PIL.Image.Image) -> torch.Tensor:\n",
    "    \"\"\"Converts a PIL image to a binary tensor.\"\"\"\n",
    "    return pil_to_tensor(image).squeeze(0).div(255).round().to(torch.int32)\n",
    "\n",
    "\n",
    "def image_to_spin_pattern(image: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Converts a binary image tensor (0s and 1s) to a spin pattern (-1s and 1s).\"\"\"\n",
    "    return (image * 2 - 1).to(torch.int32)\n",
    "\n",
    "\n",
    "def spin_pattern_to_image(tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Converts a spin pattern back to a binary image tensor.\"\"\"\n",
    "    return ((tensor + 1) / 2).to(torch.int32)\n",
    "\n",
    "\n",
    "def obscure(image: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Obscures the bottom half of an image.\"\"\"\n",
    "    rows = image.shape[0]\n",
    "    obscured_image = image.clone()\n",
    "    obscured_image[rows // 2 :, ...] = 0\n",
    "    return obscured_image\n",
    "\n",
    "\n",
    "def display(images: torch.Tensor | list[torch.Tensor], title: str | None = None) -> None:\n",
    "    \"\"\"Displays a single image or a grid of images.\"\"\"\n",
    "    if isinstance(images, list):\n",
    "        images = torch.stack(images)\n",
    "\n",
    "    if images.min() < 0:\n",
    "        images = spin_pattern_to_image(images)\n",
    "\n",
    "    match images.ndim:\n",
    "        case 3:\n",
    "            grid = (\n",
    "                make_grid(images.unsqueeze(1) * 255, pad_value=255)\n",
    "                .permute((1, 2, 0))\n",
    "                .numpy()\n",
    "            )\n",
    "            plot = plt.imshow(grid, cmap=\"grey\")\n",
    "\n",
    "        case 2:\n",
    "            plot = plt.imshow(images.numpy(), cmap=\"grey\")\n",
    "\n",
    "        case other:\n",
    "            raise ValueError(f\"Invalid input dimensionality. Expected 2 or 3-dimensional tensor, got {other}\")\n",
    "\n",
    "    plot.axes.set_title(title or \"\")\n",
    "    plot.axes.set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b88e280a0fa79da",
   "metadata": {},
   "source": [
    "## Part 1: The Classical Hopfield Network\n",
    "\n",
    "The classical model operates based on an **energy function**. The network's state is represented by a vector of \"spins\" (neurons), each being either +1 or -1. The stored patterns are considered stable, low-energy states. When a new pattern is presented, the network updates its state iteratively to descend the energy landscape until it settles into the nearest energy minimum, which corresponds to one of the stored memories.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1.  **Energy Function**: The energy $E$ of a state $\\sigma$ is given by a quadratic formula:\n",
    "    $$E = -\\frac{1}{2}\\sum_{i,j} T_{ij}\\sigma_i\\sigma_j$$\n",
    "    The network seeks to find a state $\\sigma$ that minimizes this value.\n",
    "\n",
    "2.  **Storing Patterns (Hebbian Learning)**: The weights $T_{ij}$ are determined by the patterns to be stored ($\\xi^1, \\xi^2, ..., \\xi^K$). The learning rule is a form of Hebb's rule: \"neurons that fire together, wire together.\" In practice, we sum the outer products of the patterns:\n",
    "    $$T_{ij} = \\sum_{\\mu=1}^{K} \\xi_i^\\mu \\xi_j^\\mu$$\n",
    "    This is what the `fit` method below implements.\n",
    "\n",
    "3.  **Retrieving Patterns (Update Rule)**: The network retrieves a pattern by updating its neurons one by one (asynchronously). A neuron $\\sigma_i$ keeps its state if it lowers the total energy, otherwise it flips. This is achieved by the rule that updates a unit in such a way that the energy of the entire configuration decreases. The `predict_async` method implements this iterative process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5808e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalHopfieldNetwork:\n",
    "    size: int\n",
    "    weights: torch.Tensor\n",
    "    biases: torch.Tensor\n",
    "\n",
    "    def __init__(self, size: int, neuron_fire_threshold: float) -> None:\n",
    "        assert 0 < neuron_fire_threshold < 1\n",
    "\n",
    "        self.size = size\n",
    "        self.weights = torch.zeros((size, size), dtype=torch.float32)\n",
    "        self.biases = torch.full((size,), neuron_fire_threshold, dtype=torch.float32)\n",
    "\n",
    "    def fit(self, data: torch.Tensor) -> Self:\n",
    "        \"\"\"Stores patterns in the network using Hebbian learning.\"\"\"\n",
    "        match data.shape:\n",
    "            case n_records, n_dimensions:\n",
    "                assert n_records > 0\n",
    "                assert n_dimensions == self.size\n",
    "\n",
    "            case other:\n",
    "                raise ValueError(f\"Invalid input shape. Expected 2-dimensional tensor (n_records x n_features), got {other}\")\n",
    "\n",
    "        assert data.dtype == torch.int32\n",
    "\n",
    "        # TODO:\n",
    "        ########## YOUR CODE GOES HERE (EXERCISE 1) ##########\n",
    "        # Your job is to fill in the Hebbian learning rule.\n",
    "        # 1. Calculate the weights using the formula T_ij = sum(xi_i * xi_j).\n",
    "        # 2. Normalize the weights by the number of records.\n",
    "        # 3. Set the diagonal of the weights matrix to 0 (no self-connections).\n",
    "        ######################################################\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, data: torch.Tensor) -> torch.Tensor:  # TODO: Remove\n",
    "        \"\"\"Synchronous update rule (less common).\"\"\"\n",
    "        assert data.shape == (self.size,)\n",
    "        assert data.dtype == torch.int32\n",
    "\n",
    "        state = data\n",
    "        new_state = torch.sign(self.weights @ state.to(torch.float32) - self.biases).to(\n",
    "            torch.int32\n",
    "        )\n",
    "\n",
    "        # Iterate until the state stabilizes.\n",
    "        while not torch.equal(state, new_state) or torch.equal(state, -new_state):\n",
    "            state = new_state\n",
    "            new_state = torch.sign(\n",
    "                self.weights @ state.to(torch.float32) - self.biases\n",
    "            ).to(torch.int32)\n",
    "\n",
    "        return state\n",
    "\n",
    "    def predict_async(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Asynchronously updates neurons to minimize energy.\"\"\"\n",
    "        assert data.shape == (self.size,)\n",
    "        assert data.dtype == torch.int32\n",
    "\n",
    "        state = data.reshape(-1, 1).to(torch.float32, copy=True)\n",
    "\n",
    "        # Iterate through each neuron and update its state.\n",
    "        for i in range(self.size):\n",
    "            # Calculate the weighted sum of inputs for neuron i.\n",
    "            preactivation = self.weights[i, :] @ state - self.biases[i]\n",
    "            # Update the neuron's state based on the sign.\n",
    "            activation = torch.sign(preactivation)\n",
    "            state[i] = activation\n",
    "\n",
    "        return state.flatten().to(torch.int32)\n",
    "\n",
    "    def predict_async_stochastic(  # TODO: Remove\n",
    "        self, data: torch.Tensor, max_iterations: int = 1000\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\"Updates neurons in a random order.\"\"\"\n",
    "        assert data.shape == (self.size,)\n",
    "        assert data.dtype == torch.int32\n",
    "\n",
    "        state = data.reshape(-1, 1).to(torch.float32, copy=True)\n",
    "\n",
    "        # Update neurons in a random order for a number of iterations.\n",
    "        for _ in range(max_iterations):\n",
    "            i = randint(0, self.size - 1)\n",
    "            preactivation = self.weights[i, :] @ state - self.biases[i]\n",
    "            activation = torch.sign(preactivation)\n",
    "            state[i] = activation\n",
    "\n",
    "        return state.flatten().to(torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddca262a655c7e7",
   "metadata": {},
   "source": [
    "### A Simple Example: Storing and Recalling 3x3 Patterns\n",
    "\n",
    "Let's test our classical network on a very simple task. We'll create two 3x3 patterns, a \"circle\" and a \"cross,\" and store them in a 9-neuron network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33501271",
   "metadata": {},
   "outputs": [],
   "source": [
    "circle = torch.tensor(((-1, -1, -1), (-1, 1, -1), (-1, -1, -1)), dtype=torch.int32)\n",
    "cross = torch.tensor(((-1, 1, -1), (1, -1, 1), (-1, 1, -1)), dtype=torch.int32)\n",
    "cross_and_circle_network_input = torch.stack((circle.flatten(), cross.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aa2d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "display([cross, circle])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382067b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_and_circle_network = ClassicalHopfieldNetwork(9, 0.3).fit(cross_and_circle_network_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09944c2dd786908",
   "metadata": {},
   "source": [
    "Now, let's see if the network can recall one of the patterns from a corrupted input. We'll start with a completely blank image and see which memory it converges to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4119056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_image = torch.full((3, 3), -1, dtype=torch.int32)\n",
    "recalled_image = cross_and_circle_network.predict_async(blank_image.flatten()).reshape(3, 3)\n",
    "display([blank_image, recalled_image], \"Crosses & Circles - example of inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f78056",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cross_and_circle_network.predict(torch.zeros(9, dtype=torch.int32)).reshape(3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5b4cc610787e04",
   "metadata": {},
   "source": [
    "The 3x3 example was trivial. Let's try a more challenging task: recalling handwritten digits from the MNIST dataset. We will store one example for each of the 10 digits (0 through 9) in the network. Then, we will take one of the digits, obscure its bottom half, and ask the network to reconstruct the original image.\n",
    "\n",
    "This is where we expect to see the limitations of the classical model's low storage capacity. Storing 10 complex patterns in a network of 784 neurons (28x28 pixels) is a difficult task for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49e14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_digit_examples(mnist: MNIST) -> dict[int, torch.Tensor]:\n",
    "    \"\"\"Extracts one example image for each digit from the MNIST dataset.\"\"\"\n",
    "    all_digits = dict()\n",
    "\n",
    "    for image, label in mnist:\n",
    "        all_digits.setdefault(label, image)\n",
    "\n",
    "        if all(i in all_digits for i in range(10)):\n",
    "            break\n",
    "\n",
    "    return all_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f4dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST(\"./data\", train=True, transform=preprocess, download=True)\n",
    "mnist_examples = extract_digit_examples(mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbbd6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display([mnist_examples[digit] for digit in range(10)], title=\"Memories stored in the network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f013af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_network_input = image_to_spin_pattern(torch.stack([digit.flatten() for digit in mnist_examples.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbccb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_network = ClassicalHopfieldNetwork(\n",
    "    size=28 * 28,\n",
    "    neuron_fire_threshold=0.5,\n",
    ").fit(mnist_network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f00ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero = image_to_spin_pattern(mnist_examples[4])\n",
    "zero_obscured = image_to_spin_pattern(obscure(mnist_examples[4]))\n",
    "zero_recalled = mnist_network.predict_async(zero_obscured.flatten()).reshape(28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ea7b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "display([zero, zero_obscured, zero_recalled], title=\"Classical Hopfield Network - MNIST digit recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae2e9218ca7ef20",
   "metadata": {},
   "source": [
    "As you can see, the classical network struggles to produce a clean reconstruction. The recalled image is noisy and distorted. This is a classic symptom of the network's low capacity—the energy minima for the 10 digits are interfering with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f274b934420c8829",
   "metadata": {},
   "source": [
    "## Part 2: The Modern Hopfield Network\n",
    "\n",
    "Modern Hopfield Networks, also known as **Dense Associative Memories**, solve the capacity problem by generalizing the energy function. Instead of a simple quadratic function, they use a function $F$ that can create a much \"sharper\" and more complex energy landscape.\n",
    "\n",
    "### The New Energy Function\n",
    "\n",
    "The energy is now defined as:\n",
    "$$E = -\\sum_{\\mu=1}^{K} F(\\xi^\\mu \\cdot \\sigma)$$\n",
    "Here, $\\xi^\\mu \\cdot \\sigma$ is the dot product (or overlap) between a stored memory and the current state. The function $F$ determines the shape of the energy wells.\n",
    "\n",
    "By choosing a rapidly growing function for $F$, like a high-degree polynomial ($F(x) = x^n$), the network can store vastly more patterns. For a polynomial of degree $n$, the capacity grows to $K_{max} = \\alpha_n N^{n-1}$.\n",
    "\n",
    "### The New Retrieval Rule\n",
    "\n",
    "With the new energy function, the update rule for a neuron $\\sigma_i$ becomes: **flip the neuron if and only if doing so decreases the total energy of the system**.\n",
    "\n",
    "The `predict_async` method in our `ModernHopfieldNetwork` class implements this by calculating the total energy of the network with neuron $i$ as `+1` and as `-1`, and then choosing the state that results in a lower energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionFunction:\n",
    "    \"\"\"A wrapper for the function F and its derivative.\"\"\"\n",
    "    function: Callable[[torch.Tensor], torch.Tensor]\n",
    "    derivative: Callable[[torch.Tensor], torch.Tensor]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        function: Callable[[torch.Tensor], torch.Tensor],\n",
    "        derivative: Callable[[torch.Tensor], torch.Tensor],\n",
    "    ) -> None:\n",
    "        self.function = function\n",
    "        self.derivative = derivative\n",
    "\n",
    "    def __call__(self, argument: torch.Tensor) -> torch.Tensor:\n",
    "        return self.function(argument)\n",
    "\n",
    "\n",
    "class PolynomialInteraction(InteractionFunction):\n",
    "    \"\"\"Interaction function F(x) = x^n.\"\"\"\n",
    "    degree: int\n",
    "\n",
    "    def __init__(self, degree: int) -> None:\n",
    "        super().__init__(\n",
    "            function=lambda x: x**degree,\n",
    "            derivative=lambda x: degree * x ** (degree - 1),\n",
    "        )\n",
    "        self.degree = degree\n",
    "\n",
    "\n",
    "class ExponentialInteraction(InteractionFunction):\n",
    "    \"\"\"Interaction function F(x) = exp(x).\"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(function=lambda x: x.exp(), derivative=lambda x: x.exp())\n",
    "\n",
    "\n",
    "class ModernHopfieldNetwork:\n",
    "    size: int\n",
    "    interaction: InteractionFunction\n",
    "    training_data: torch.Tensor\n",
    "    weights: torch.Tensor\n",
    "    biases: torch.Tensor\n",
    "\n",
    "    def __init__(self, size: int, neuron_fire_threshold: float, interaction: InteractionFunction) -> None:\n",
    "        assert 0 < neuron_fire_threshold < 1\n",
    "\n",
    "        self.size = size\n",
    "        self.interaction = interaction\n",
    "        self.training_data = torch.zeros((1, size), dtype=torch.int32)\n",
    "        self.weights = torch.zeros((size, size), dtype=torch.float32)\n",
    "        self.biases = torch.full((size,), neuron_fire_threshold, dtype=torch.float32)\n",
    "\n",
    "    def fit(self, data: torch.Tensor) -> Self:\n",
    "        \"\"\"Stores the patterns. For this model, 'fitting' is just memorizing the data.\"\"\"\n",
    "        match data.shape:\n",
    "            case n_records, n_dimensions:\n",
    "                assert n_records > 0\n",
    "                assert n_dimensions == self.size\n",
    "\n",
    "            case other:\n",
    "                raise ValueError(f\"Invalid input shape. Expected 2-dimensional tensor (n_records x n_features), got {other}\")\n",
    "\n",
    "        assert data.dtype == torch.int32\n",
    "        self.training_data = data.to(torch.float32, copy=True)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def energy(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Calculates the energy of a given state.\"\"\"\n",
    "\n",
    "        # TODO:\n",
    "        ########## YOUR CODE GOES HERE (EXERCISE 2) ##########\n",
    "        # Your job is to fill in the modern energy function.\n",
    "        # 1. Calculate the overlaps (projections) of the state onto each memory pattern.\n",
    "        # 2. Apply the interaction function F to the vector of overlaps.\n",
    "        # 3. Sum the results and negate to get the final energy.\n",
    "        ######################################################\n",
    "        pass\n",
    "\n",
    "    def predict_async(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Updates neurons by directly minimizing the energy function.\"\"\"\n",
    "        assert data.shape == (self.size,)\n",
    "        assert data.dtype == torch.int32\n",
    "\n",
    "        state = data.reshape(-1, 1).to(torch.float32, copy=True)\n",
    "\n",
    "        # TODO:\n",
    "        ########## YOUR CODE GOES HERE (EXERCISE 3) ##########\n",
    "        # Your job is to fill in the modern update rule.\n",
    "        # For each neuron i in the network:\n",
    "        #   1. Calculate the network's energy if neuron i were +1.\n",
    "        #   2. Calculate the network's energy if neuron i were -1.\n",
    "        #   3. Set the state of neuron i to the value that resulted in lower energy.\n",
    "        ######################################################\n",
    "\n",
    "        return state.flatten().to(torch.int32)\n",
    "    \n",
    "    def predict_async_stochastic(self, data: torch.Tensor, max_iterations: int) -> torch.Tensor:  # TODO: Remove\n",
    "        \"\"\"Updates neurons in random order by directly minimizing energy.\"\"\"\n",
    "        assert data.shape == (self.size,)\n",
    "        assert data.dtype == torch.int32\n",
    "\n",
    "        state = data.reshape(-1, 1).to(torch.float32, copy=True)\n",
    "\n",
    "        for _ in range(max_iterations):\n",
    "            i = randint(0, self.size - 1)\n",
    "\n",
    "            state[i] = 1\n",
    "            energy_with_one = self.energy(state)\n",
    "\n",
    "            state[i] = -1\n",
    "            energy_with_minus_one = self.energy(state)\n",
    "\n",
    "            state[i] = 1 if energy_with_one < energy_with_minus_one else -1\n",
    "\n",
    "        return state.flatten().to(torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7261b6e19e63fc61",
   "metadata": {},
   "source": [
    "### Testing the Modern Network on MNIST\n",
    "\n",
    "Now, let's repeat the exact same experiment with our `ModernHopfieldNetwork`. We will use a `PolynomialInteraction` with a degree of 15. This creates a much sharper energy landscape, which should allow the network to distinguish between the stored digits far more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_modern_network = ModernHopfieldNetwork(\n",
    "    size=28 * 28,\n",
    "    neuron_fire_threshold=0.5,\n",
    "    interaction=PolynomialInteraction(15),\n",
    ").fit(mnist_network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db96292",
   "metadata": {},
   "outputs": [],
   "source": [
    "digit = image_to_spin_pattern(mnist_examples[4])\n",
    "digit_obscured = image_to_spin_pattern(obscure(mnist_examples[4]))\n",
    "digit_recalled = mnist_modern_network.predict_async(digit_obscured.flatten()).reshape(28, 28)\n",
    "display([digit, digit_obscured, digit_recalled], title=\"Modern Hopfield Network - MNIST digit recall\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern-hopfield-networks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
